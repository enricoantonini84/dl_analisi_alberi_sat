\chapter{Technical Details}

\section{What is a Neural Network}
A neural network is a model that simulates the functioning of the human brain, so that it can learn to recognize images and texts, thanks to a learning process (training) and then subsequently give feedback and predictions about similar or same class elements, with a process called inference.\\
It is good to specify that not all models are based on neural networks, in fact there are non-neural algorithms that can simulate human cognitive reasoning, such as decision trees: they are the perfect example of models categorized as machine learning, as they learn from the training data provided in inputs, which are not composed of layers and nodes as in traditional neural networks\cite{Choi2020}.\\
Specifically, the multi-layer neural networks, or multi-layers, covered in this chapter are a particular subset of machine learning called deep learning.

\subsection{Layers}
A model is composed of several sequentially organized layers, called layers, made by nodes that can be considered as artificial neurons.\\
Simplifying as much as possible, layers can be input, output, or intermediate:
\begin{itemize}
\item Input layer: takes input data. If we use a YOLO model for example, it will be an image.
\item hidden layers: receive input, process information through mathematical calculations and functions, and pass results to the next layer. They are the inner layers to the model.
\item output layer: Returns the final prediction of the model. In our case, it tells us if it found objects within the image, possibly also specifying additional information such as classes and confidence level.
\end{itemize}
This layered structure allows the model to transform and combine data, learning to recognize patterns and objects. Each layer adds a new transformation, allowing the model to learn increasingly sophisticated functions\cite{ibm_layers}.

\subsection{Weights}
Each link between consecutive layer nodes is associated with a weight, which is a numerical value that indicates the strength of the connection between two nodes. During training, these weights are continuously updated so that, thanks to the passage of data through layers, the model can combine inputs more and more effectively to recognize patterns and make accurate predictions.\\
Weights work on information and data, which are transmitted from one node to the next. Specifically a high weight amplifies the information, while a low weight attenuates it: it is precisely the optimization of these weights that allow the model to improve its accuracy and predictive ability\cite{googlemlglossary_weights}.

\section{Dataset}
A dataset is a structured collection of data, in the case of this research it is mainly formed by images with annotations and is organized to be analyzed and processed by machine learning algorithms.
Datasets can be composed of:
\begin{itemize}
    \item Training set: images used for model training.
    \item Validation set: images used during training, to evaluate the performance of the model on unseen data and to perform parameter tuning.
    \item Test set: a set of images used at the end of training, to evaluate the final performance of the model, on completely new data that was not used for training.
\end{itemize}
The validation set is optional, but very useful for improving the quality of the training of the model.

\subsection{Importance of dataset}
The quality and consistency of the data in the dataset is as important as the algorithm itself: from a dataset with clean images and well-made annotations it is possible to do training (or transfer learning) and train an accurate model.\\
The dataset must also contain images in the format provided by the model specifications (e.g. YOLO in 640x640)\\
The subdivision into training, validation and test sets allows to develop a robust model and test its performance on data "never seen before"\\

\subsection{Manual Annotation}
To build a dataset suitable for the object recognition theme, which is the purpose of this research, you need to have images with resolution, number of channels (e.g. RGB) and format, which are compatible with the model to be trained.\\
Depending on the neural network to be used then, the images must be accompanied by a file describing the content or object to be identified: for example for YOLO models each image is associated with a file, which contains coordinates and class of each object present within the image.\\
At this point the neural network that underlies the model, during the training procedure, will learn to distinguish the objects that we have annotated in the dataset and precisely thanks to this procedure, will then be able to evaluate in a more or less precise way, depending on the type of network and the quality of the training dataset, whether or not any image provided to it in input contains objects similar to those annotated, also providing values that indicate the estimate of how precise that detection is.\\
Platforms like Roboflow offer an editor where you can annotate images and simultaneously generate files with annotations.

\subsection{Searching for a dataset with annotated images}
A second solution is to use pre-annotated datasets, which can be downloaded from Roboflow or GitHub repositories.\\
In this case it is crucial to make sure that the dataset has been created specifically for the network you want to train and that the annotations are correct. In general, the datasets made available online, if from computer vision projects and documented in research articles, are quite reliable.

\section{Training}
For a machine learning model to learn to recognize significant patterns or features in an image, it is necessary to perform an operation called training, which is in fact a real training of the model.\\
Without proper training the neural network would not have a strategy to interpret the data and perform the desired task.\\\\
The training is carried out starting from a dataset, or a collection of images with their "annotations": in each image of a dataset the position and class of the object is highlighted, which is indispensable to train the network to subsequently recognize these objects in the images that we will have to classify.\\
There are two types of training that can be performed on a network, full training (from scratch) and transfer learning\cite{isong2025buildingefficientlightweightcnn}.

\subsection{Training from scratch}In this type of training, the model is fully trained on annotated data without starting from pre-trained weights. Weights are numerical values that are assigned to connections between nodes in the neural network: whenever an input arrives at a node, it is multiplied by the weight associated with that specific connection, the sum of all these inputs will then determine its output values.\\During the neural network training phase, weights are constantly updated using learning algorithms, so as to minimize the difference between model predictions and real values.\\Starting from scratch, these weights are not defined but random numbers are assigned that, only during the training process, will be modified to optimize performance.\\This type of training is very valid when we have available a considerable number of annotated data and the use of the model is very specific.\subsection{Transfer learning}When the dataset is limited, or if the goal is to save time and resources, it is possible to start from a pre-trained model.\\The idea is to "transfer" to a model provided with weights and with already significant precision parameters, the part of knowledge necessary to recognize the annotated data in our dataset, we then start from the pre-trained model and adapt it (fine-tuning) to the target, often "freezing" the first layers and optimizing only the final ones.\section{Training parameters}Below are listed some parameters, which are normally specified during model training: they are useful for honing the learning process and are also called "Hyperparameters".\\These parameters are rather generic, because they are common to many training procedures even between different algorithms\cite{googlemlglossary}.\\

\subsection{Epoch}An epoch represents a complete passage of the dataset through the algorithm being learned, it is a fundamental concept in the training of neural networks, which allows the model to learn by constantly continuing to review the same images, or the same elements of the dataset\cite{ultralytics_epoch}.\\The epoch number is usually specified in each training procedure and determines how many times the model will learn from the dataset train set, affecting the performance and quality of the model.\\Choosing the correct epoch number is very important because if it is chosen incorrectly it can create two problems:\begin{itemize}
    \item Underfitting: the model was not trained for a sufficient number of epochs. Performances are not adequate on either the training set or the test set\item Overfitting: the model has been trained for too many epochs. In this case he stores the training data too thoroughly and loses his ability to analyze the new data. It has excellent precision on the training set but poor on the validation dataset.\end{itemize}A common technique to avoid overfitting is that of early stopping, in which training is stopped when performance on the validation set stops improving. Sometimes it is possible to specify a parameter called patience, which specifies after how many "non-improvement" epochs the training must stop.\subsection{Batch size}Batch size is another important parameter that defines the number of samples processed before the model's internal parameters are updated. Samples are in this case the images, which are used for model training.\\By dividing the training set into batches, i.e. reduced subsets, avoiding processing all data simultaneously, which could be computationally problematic in terms of processing speed and occupied memory.\\Choosing a batch size too low though, implies a reduced training speed, as the weights of the model are updated much more frequently.\subsection{Loss function}It is a fundamental mathematical function in machine learning, which measures how far the predictions generated by the model deviate from real values.\begin{equation*}
\mathrm{Loss}f(prediction, realValue)\end{equation*}The function is called both during training for each batch of images, and during validation at the end of each epoch. The value then returned by the loss function, during training, can be used by optimizers that allow you to update parameters, such as weights, to each epoch as a sort of "automatic pilot".\section{Hardware needed for training}Machine learning models need special hardware resources to be able to be trained because, using a normal desktop computer that uses the CPU alone to "instruct" the model, is too limiting not so much for the frequency but for the insufficient number of cores it has on board, it is instead necessary to use hardware that allows parallel computing: for this task the GPUs come to our aid.\\The latter, in fact, especially those produced by Nvidia, are compatible with the parallel computing platform CUDA (Compute Unified Device Architecture) that takes advantage of the hundreds or thousands of cores on board the chips, thus managing to perform massive mathematical calculations and manage huge amounts of data\cite{whatiscuda}.\\Importantly, heavier machine learning models require a lot of RAM, and it is therefore necessary to equip yourself with GPUs with sufficient video memory (VRAM) on board, to ensure that the model can remain fully resident in memory during training or inference procedures.\subsection{Bare metal}To use CUDA on a local machine you need to buy the physical hardware, which generally in the consumer sphere is an expansion board on PCIe buses that allows the connection of one or more monitors and has the task of accelerating calculations that would be too heavy for the CPU, such as calculating polygons and shaders in games or training/inference on machine learning models.\\In the industrial sector, however, there is room for solutions not designed for the recreational sector, which are instead marketed as parallel computing accelerators.\\There are also reduced versions on embedded/single board computers (e.g. Jetson Orin) that allow the use of CUDA on the edge, for example to evaluate images from a camera.\\All of these solutions involve the purchase of dedicated hardware, which needs to be configured correctly in order to work and, especially the most performing GPU models with high VRAM have a major cost.\\Python is usually used for training and inference, as it has libraries suitable for working with models (such as Pytorch or TensorFlow) and it is necessary that the dedicated code is developed.\subsection{Cloud Resources}A real alternative to on-premise processing is to use cloud services that result in hardware cost savings, as it is all left to the managed platform, but still imply the need to work from Python sources for training or inference with models.\\There is usually a monthly budgetable cost with a calculator, which varies depending on the service and hardware made available by the cloud platform.\\The most famous providers are Google Cloud Platform, AWS EC2 and Azure VMs, which offer Linux or Windows virtual machines with GPUs.\\Alternatively, at a lower cost, services such as Google Colab are available that only support scripting languages (e.g. Python, R) offering a monthly fee access to a number of CPU+GPU processing hours. They are a good compromise to work with machine learning models.\\

\subsection{All-in-one solutions}One solution that is taking hold in recent years is to rely on all-in-one services that, against a monthly fee, provide for the management of the dataset and the creation of the model through training. This type of service also offers the ability to access online models, via REST services, with endpoints that allow you to inference without having to worry about developing scripts to run on your machine or in the cloud.\\Among the most popular services are definitely Roboflow and Ultralytics.\section{Comparing Models}At the time of writing this document (ca. end 2025) several solutions are available, which promise to be effective in identifying objects within an image, in a more or less elaborate way.\\Unlike neural networks that became popular in the first half of the 1920s (e.g. ChatGPT, Gemini, Llama or Grok) to be very effective with natural language (NLP) given their nature of LLM, acronym of Large Language Model, in this specific case the research will deal with neural networks for computer vision, such as recursive and non-recursive convolutional networks.\subsection{AdaBoost}It is a machine learning algorithm that combines multiple models called weak learners to create a more accurate evolved model\cite{bejabattais2023overviewadaboostreconciling}. Weak learners are slightly better performing models or algorithms than a random prediction, for example in a binary classification they reach just over 50\%of precision. By doing sequential training on these weak learners and combining them, the final model will be nothing more than the weighted sum of all the smaller models, resulting in more performing and robust.\\In the case of AdaBoost the process is adaptive because it corrects errors made in the first round of training, thus making it more effective.\\It is important, as previously mentioned, to emphasize that AdaBoost is not a neural network but, by its nature by combining smaller models, it is "simply" a decision tree.\\DetecTree uses AdaBoost to classify, pixel by pixel, what is part of a tree and what is not.\subsection{CNN}Some object detection algorithms are based on a convolutional neural network (CNN)\cite{oshea2015introductionconvolutionalneuralnetworks}, which is used to allow detection of objects in the image. It is a specific type of deep learning algorithm, designed primarily to analyze visual data, such as images and videos. These algorithms are categorized as deep learning, precisely because of their characteristic of having multiple levels.\\In practice it mimics the functionality of the human cerebral cortex, automatically extracting certain characteristics or objects, such as trees, from the image: this is possible through filters called convolutionals, which go in search of specific patterns.\\The name of the filters comes from the type of operation that is carried out on the input images, that is the convolution: it is a mathematical operation that in simple terms consists of sliding a filter (also called kernel) over the input image. The filter examines the image and calculates a weighed combination of pixel values, producing an output value. Repeating the process several times creates a new image that is able to highlight specific features such as edges, angles or textures: in essence it then allows the network to recognize visual patterns efficiently while also maintaining the spatial position of the detected objects. An example of CNN is the YOLO network.\subsection{R-CNN}In some cases networks can also be R-CNN, i.e. Region-based CNN which, like normal convolutional networks, are also deep learning algorithms.\\Again they are models designed for object detection: first multiple proposals of regions (areas potentially containing objects) are generated through specific algorithms, then a CNN is applied to each region to extract characteristics, and finally objects are classified and located in those regions\cite{r-cnn}. In essence, no filters are applied to the whole image, but convolution operations are carried out locally in distinct regions.\\Among the R-CNNs we can find DetecTree2.\subsection{ViT}The commercial LLM models mentioned at the beginning of this section, which in recent years have undergone an incredible evolution, have introduced an architecture called a transformer.\\As opposed to (R)CNNs seen earlier, transformers rely on self-attention mechanisms where some of the input data, such as a word or image, is compared and weighed against all other parts to understand which ones are most relevant in the current context. In this way the model can assess the importance of the different parts of a sequence, regardless of their position\cite{vaswani2023attentionneed}. A Transformer consists of:\begin{itemize}
\item Encoder: Converts text to input in an intermediate representation. The encoder is a neural network.\item Decoder converts the intermediate representation into a text output. The decoder is also a neural network.\end{itemize}Recently, the Transformer architecture has also been applied to networks dedicated to vision, thus creating Vision Transformers (ViTs): instead of treating the image like a pixel grid, as is the case with (R)CNNs, the ViT splits the image into small pieces of fixed size, which are then transformed into vectors (embeddings), are enriched with spatial position information and then switched to a transformer encoder, which uses both global and local mechanisms.\\This process improves performance in tasks such as classification, segmentation, and object recognition, especially with large training datasets. Segformer is a neural network based on this deep learning algorithm.\section{Performance measurement}Models, once trained, can be evaluated based on certain metric specifications, which are also common across different neural networks.\\Below are listed several general metrics, the specific metrics and dedicated to precise models, are instead reported in the following chapters.\subsection{Precision}Indicates the percentage of objects identified by the model that are actually correct. In other words, it indicates, on all the times the model has found an object in an image, how many times it has actually "got right". High accuracy means few false positives.\subsection{Recall}It measures the percentage of objects actually present in the image that were found by the model, basically it returns us a value relative to the number of objects recognized compared to how many were currently annotated as good in the dataset. A tall recall is equivalent to a few false negatives.\subsection{Intersection over Union (IoU)}Fundamental computer vision metric in object recognition, which measures how much two rectangles (boxes) overlap with each other relative to their extension. It is used in practice to compare how much the predicted bounding box, coincides with the real bounding box (called ground truth).\begin{equation*}IoU\frac{areaIntersection}{areaUnion},\quadIoU\in [0,1]
\end{equation*}for example an IoU of 0.6 indicates that two boxes have the 60\%of shared area\cite{mAPV7}.\\The IoU threshold is an input value of the model, which establishes how much an object must be "centered" to be considered correct the prediction: the higher the value, the stricter the valuation.\subsection{Mean Average Precision (mAP)}Parameter used to measure the performance of computer vision models, such as YOLO or DetecTree2. The mAP represents the average accuracy on different IoU thresholds, of all classes present in the dataset: it is calculated as the arithmetic mean of the accuracy values of all classes, providing an overall measure of the model's ability to classify objects correctly. It is also crucial to compare different models that are involved in carrying out the same task, or even different versions of the same model.\begin{equation*}
\mathrm{mAP} = \frac{1}{N} \sum_{i=1}^{N}AP_i,\quadmAP\in [0,1]
\end{equation*}with N number of classes and APi that corresponds to the average accuracy of class i\cite{mAPV7}.

\subsection{Mean Average Precision 0.50 (mAP50)}Metric that calculates the Average Precision across all classes, using a fixed IoU threshold of 0.50. A prediction is considered correct (true positive) only if the overlap with the ground truth is at least 50\%.\\This metric provides a model performance assessment with a tolerance threshold, and is useful for applications where an approximate object location is sufficient.\subsection{Mean Average Precision 0.50-0.95 (mAP50-95)}Metric that calculates the Average Precision by considering multiple thresholds of IoU, 0.50 to 0.95 in increments of 0.05 (total 10 thresholds). For each threshold the mAP is calculated, then the average of all these values is made.\\This metric is much more rigorous than mAP50, because it evaluates the model's accuracy on different levels of difficulty, requiring both approximate and very precise localizations. A high mAP50-95 indicates that the model is accurate in all contexts, from approximate detection to millimeter accuracy.\subsection{F1 score}F1 is the harmonic mean of precision and recall\begin{equation*}
F1 = 2 *\frac{precision + recall}{precision * recall},\quad F1 \in [0,1]
\end{equation*}this value balances precision and recall, penalizing strongly the case where one of the two is very low, in fact the closer you get to 1 the less you have false positives and false negatives. This value is very important because it tells us how precise and complete a model is in its predictions.